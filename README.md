# YOLO v1 Object Detection

![Detection Examples](assets/prediction_examples.png)

[![Tests](https://github.com/mattiaskvist/yolo-v1/actions/workflows/pytest.yml/badge.svg)](https://github.com/mattiaskvist/yolo-v1/actions/workflows/pytest.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![arXiv](https://img.shields.io/badge/arXiv-1506.02640-b31b1b.svg)](https://arxiv.org/abs/1506.02640)

[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.8+](https://img.shields.io/badge/pytorch-2.8+-red.svg)](https://pytorch.org/)
[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

[![GitHub stars](https://img.shields.io/github/stars/mattiaskvist/yolo-v1?style=social)](https://github.com/mattiaskvist/yolo-v1/stargazers)

A clean, modular PyTorch implementation of **You Only Look Once (YOLO) v1**, the pioneering single-shot object detection model. This implementation features a flexible architecture with support for multiple backbones, distributed training, and comprehensive evaluation metrics.

## Overview

YOLO v1 revolutionized object detection by framing it as a single regression problem, directly predicting bounding boxes and class probabilities from full images in one evaluation. This implementation stays true to the original paper while incorporating modern best practices and engineering improvements.

### Key Features

- **üèóÔ∏è Modular Architecture**: Clean separation of concerns with pluggable backbone networks
- **üîÑ Transfer Learning**: Support for pretrained ResNet50 backbone with configurable freezing
- **‚òÅÔ∏è Distributed Training**: Seamless integration with [Modal](https://modal.com/) for cloud GPU training
- **üéØ Auto Device Detection**: Automatically selects optimal device (MPS for Apple Silicon, CUDA for NVIDIA, CPU fallback)
- **üìä Comprehensive Metrics**: Full mAP@50:95 evaluation with per-class and size-based analysis
- **üß™ Production Ready**: Extensive test coverage and type hints throughout
- **üìà Training Monitoring**: TensorBoard integration for real-time training visualization

## Performance

Evaluated on PASCAL VOC 2007 test set (4,952 images):

| Metric | Score |
|--------|-------|
| **mAP@50:95** | 21.20% |
| **mAP@0.5** | 43.87% |
| **mAP@0.75** | 14.33% |
| **Precision** | 17.47% |
| **Recall** | 52.27% |

### Top Performing Classes (AP@0.5)

| Class | AP@0.5 | Class | AP@0.5 |
|-------|--------|-------|--------|
| Cat | 69.54% | Dog | 67.83% |
| Train | 60.29% | Horse | 61.97% |
| Bus | 50.29% | Bicycle | 54.35% |

### Performance by Object Size

| Size | mAP@50:95 | mAP@0.5 | Count |
|------|-----------|---------|-------|
| Large (‚â•96√ó96) | 26.19% | 56.39% | 8,322 |
| Medium (32√ó32-96√ó96) | 9.25% | 9.94% | 4,126 |
| Small (<32√ó32) | 8.18% | 8.18% | 875 |

## Installation

### Prerequisites

- [uv](https://github.com/astral-sh/uv)

### Quick Install

```bash
# Clone the repository
git clone https://github.com/mattiaskvist/yolo-v1.git
cd yolo-v1

# Install dependencies with uv
uv sync
```

### Optional: Modal Setup for Cloud Training

```bash
# Authenticate (follow prompts)
uv run modal setup
```

Make sure you add `KAGGLE_USERNAME` and `KAGGLE_KEY` as secrets in your Modal project for dataset downloads.

## Usage

### Training

#### Local Training

```bash
# Train with auto device detection (MPS/CUDA/CPU)
uv run src/train.py --epochs 135

# Train on specific device
uv run src/train.py --epochs 135 --device mps   # Apple Silicon
uv run src/train.py --epochs 135 --device cuda  # NVIDIA GPU
uv run src/train.py --epochs 135 --device cpu   # CPU only
```

#### Cloud Training with Modal

```bash
# Train on Modal with GPU (L4)
uv run modal run -d src/train.py --epochs 135 --remote

# Resume from checkpoint (if interrupted)
uv run modal run -d src/train.py --resume true --epochs 135 --remote
```

### Evaluation

```bash
# Evaluate model on VOC2007 test set
uv run src/evaluate.py --checkpoint checkpoints/yolo_best.pth

# Evaluate with specific device
uv run src/evaluate.py --checkpoint checkpoints/yolo_best.pth --device mps
```

Note: if trained on Modal, download the checkpoint from the `checkpoints/` directory in your Modal project.

### Inference

```bash
# Run inference on images
uv run src/predict.py --checkpoint checkpoints/yolo_best.pth --image-dir path/to/images/

# Adjust confidence and NMS thresholds
uv run src/predict.py \
    --checkpoint checkpoints/yolo_best.pth \
    --image-dir path/to/images/ \
    --conf-threshold 0.3 \
    --nms-threshold 0.4
```

## Project Structure

```txt
yolo-v1/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ train.py              # Training script with Modal integration
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py           # Model evaluation on test set
‚îÇ   ‚îú‚îÄ‚îÄ predict.py            # Inference on images
‚îÇ   ‚îî‚îÄ‚îÄ yolo/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py       # Public API exports
‚îÇ       ‚îú‚îÄ‚îÄ models.py         # YOLO architecture & backbones
‚îÇ       ‚îú‚îÄ‚îÄ loss.py           # YOLO loss function
‚îÇ       ‚îú‚îÄ‚îÄ dataset.py        # VOC dataset wrapper
‚îÇ       ‚îú‚îÄ‚îÄ inference.py      # Inference engine with NMS
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py        # mAP evaluation metrics
‚îÇ       ‚îú‚îÄ‚îÄ training/         # Training utilities
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py    # Training loops
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ logging.py    # Console & TensorBoard logging
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ checkpoints.py # Model checkpoint management
‚îÇ       ‚îî‚îÄ‚îÄ utils/
‚îÇ           ‚îî‚îÄ‚îÄ visualization.py # Visualization utilities
‚îú‚îÄ‚îÄ tests/                    # Test suite
‚îú‚îÄ‚îÄ checkpoints/              # Model checkpoints
‚îî‚îÄ‚îÄ predictions/              # Inference outputs
```

## Architecture

### Model Overview

- **Grid Size (S)**: 7√ó7
- **Bounding Boxes per Cell (B)**: 2
- **Classes (C)**: 20 (PASCAL VOC)
- **Input Size**: 448√ó448 RGB
- **Output**: 7√ó7√ó30 tensor (per cell: 2 boxes √ó 5 predictions + 20 class probabilities)

### Backbone Options

1. **YOLOv1Backbone**: Original architecture from the paper
   - 24 convolutional layers
   - Alternating 1√ó1 and 3√ó3 convolutions
   - Leaky ReLU activation (Œ±=0.1)

2. **ResNetBackbone**: Transfer learning with ResNet50
   - Pretrained on ImageNet
   - Configurable layer freezing
   - Faster convergence

## Dataset

Training and evaluation use **PASCAL VOC 2007 and 2012** datasets:

- **Training**: VOC 2007 trainval + VOC 2012 trainval (~16,551 images)
- **Validation**: VOC 2007 test (4,952 images)
- **Classes**: 20 object categories (person, car, dog, etc.)

Datasets are automatically downloaded from Kaggle during first training.

## Acknowledgments

- **Original Paper**: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) by Joseph Redmon et al.
